{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "# ----- Hyperparameters ----- #\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 8e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ROOT = '../celeb_dataset/images'\n",
    "IMAGE_SIZE = 128\n",
    "MASK_SIZE = 64\n",
    "GENERATOR_CHECKPOINT = 'generator.pth'\n",
    "DISCRIMINATOR_CHECKPOINT = 'discriminator.pth'\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "VISUALIZE_TRAINING = False\n",
    "print(f'{DEVICE=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms, image_size, mask_size, mode='train'):\n",
    "        self.files = os.listdir(root) # generates a list of file names\n",
    "        split_size = int(len(self.files) * 0.2) # 0.2 is for test-split size\n",
    "        self.files = self.files[:-split_size] if mode == 'train' else self.files[-split_size:]\n",
    "        self.mode = mode\n",
    "        self.transform = transforms\n",
    "        self.image_size = image_size\n",
    "        self.mask_size = mask_size\n",
    "        \n",
    "    def apply_random_mask(self, image):\n",
    "        x1,y1 = np.random.randint(0, self.image_size - self.mask_size, 2)\n",
    "        x2,y2 = x1 + self.mask_size, y1 + self.mask_size\n",
    "        \n",
    "        mask_true_value = image[:, y1:y2, x1:x2] # (batch, x1 to x2, y1 to y2)\n",
    "        masked_image = image.clone()\n",
    "        masked_image[:, y1:y2, x1:x2] = 1 # sets the masked region to 1\n",
    "        \n",
    "        return masked_image, mask_true_value\n",
    "    \n",
    "    def apply_center_mask(self, image):\n",
    "        upper_left = (self.image_size-self.mask_size)//2\n",
    "        \n",
    "        mask_true_value = image[:, upper_left:upper_left+self.mask_size, upper_left:upper_left+self.mask_size]\n",
    "        masked_image = image.clone()\n",
    "        masked_image[:, upper_left:upper_left+self.mask_size, upper_left:upper_left+self.mask_size] = 1\n",
    "        \n",
    "        return masked_image, mask_true_value\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_file = self.files[index]\n",
    "        image = Image.open(os.path.join(ROOT, image_file))\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            masked_image, mask_true_value = self.apply_random_mask(image)\n",
    "            \n",
    "        else:\n",
    "            masked_image, mask_true_value = self.apply_center_mask(image)\n",
    "            \n",
    "        return masked_image, mask_true_value\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    ImageDataset(ROOT, transforms_, IMAGE_SIZE, MASK_SIZE, mode='train'),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    ImageDataset(ROOT, transforms_, IMAGE_SIZE, MASK_SIZE, mode='test'),\n",
    "    batch_size=5,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "training_visualization_images = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3, features=64):\n",
    "        super().__init__()\n",
    "        # input size is (batch, 3, 128, 128)\n",
    "        self.downsample_layers = nn.Sequential(\n",
    "            nn.Conv2d(channels, features, kernel_size=4, stride=2, padding=1), # (batch, 64, 64, 64)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features, features, kernel_size=4, stride=2, padding=1), # (batch, 64, 32, 32)\n",
    "            nn.BatchNorm2d(features),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features, features*2, kernel_size=4, stride=2, padding=1), # (batch, 128, 16, 16)\n",
    "            nn.BatchNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features*2, features*4, kernel_size=4, stride=2, padding=1), # (batch, 256, 8, 8)\n",
    "            nn.BatchNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features*4, features*8, kernel_size=4, stride=2, padding=1), # (batch, 512, 4, 4)\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features*8, 4000, kernel_size=4, stride=1, padding=0), # (batch, 4000, 4, 4)\n",
    "        )\n",
    "        \n",
    "        self.upsample_layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4000, features*8, kernel_size=4, stride=1, padding=0), # (batch, 512, 4, 4)\n",
    "            nn.BatchNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(features*8, features*4, kernel_size=4, stride=2, padding=1), # (batch, 256, 8, 8)\n",
    "            nn.BatchNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(features*4, features*2, kernel_size=4, stride=2, padding=1), # (batch, 128, 16, 16)\n",
    "            nn.BatchNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(features*2, features, kernel_size=4, stride=2, padding=1), # (batch, 64, 32, 32)\n",
    "            nn.BatchNorm2d(features),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(features, channels, kernel_size=4, stride=2, padding=1), # (batch, 64, 64, 64)\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.downsample_layers(x)\n",
    "        x = self.upsample_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3, features=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(channels, features, kernel_size=4, stride=2, padding=1), # (batch, 64, 32, 32)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features, features*2, kernel_size=4, stride=2, padding=1), # (batch, 128, 16, 16)\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features*2, features*4, kernel_size=4, stride=2, padding=1), # (batch, 256, 8, 8)\n",
    "            nn.InstanceNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features*4, features*8, kernel_size=4, stride=2, padding=1), # (batch, 512, 4, 4)\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(features*8, 1, kernel_size=4, stride=1, padding=0), # (batch, 1, 1, 1)\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(DEVICE)\n",
    "discriminator = Discriminator().to(DEVICE)\n",
    "\n",
    "adversarial_loss = nn.BCELoss().to(DEVICE)\n",
    "pixelwise_loss = nn.L1Loss().to(DEVICE)\n",
    "\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_batch(masked_image, mask_true_value):\n",
    "    optimizer_generator.zero_grad()\n",
    "    \n",
    "    generated_mask_true_value = generator(masked_image)\n",
    "    discriminator_on_generated = discriminator(generated_mask_true_value)\n",
    "    \n",
    "    adversarial_loss_value = adversarial_loss(discriminator_on_generated, torch.ones_like(discriminator_on_generated))\n",
    "    pixelwise_loss_value = pixelwise_loss(generated_mask_true_value, mask_true_value)\n",
    "    \n",
    "    generator_loss = 0.001 * adversarial_loss_value + 0.999 * pixelwise_loss_value\n",
    "    \n",
    "    generator_loss.backward()\n",
    "    optimizer_generator.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator_batch(masked_image, mask_true_value):\n",
    "    optimizer_discriminator.zero_grad()\n",
    "    \n",
    "    generated_mask_true_value = generator(masked_image).detach()\n",
    "    \n",
    "    discriminator_on_generated = discriminator(generated_mask_true_value)\n",
    "    discriminator_on_real = discriminator(mask_true_value)\n",
    "    \n",
    "    generated_loss = adversarial_loss(discriminator_on_generated, torch.zeros_like(discriminator_on_generated))\n",
    "    real_loss = adversarial_loss(discriminator_on_real, torch.ones_like(discriminator_on_real))\n",
    "    \n",
    "    discriminator_loss = 0.5 * (generated_loss + real_loss)\n",
    "    discriminator_loss.backward()\n",
    "    optimizer_discriminator.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(masked_image, mask_true_value, upper_left_x, upper_left_y):\n",
    "    filled_image = masked_image.clone()\n",
    "    filled_image[:, :, upper_left_y:upper_left_y+MASK_SIZE, upper_left_x:upper_left_x+MASK_SIZE] = mask_true_value\n",
    "    \n",
    "    return filled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename):\n",
    "    print('\\nsaving checkpoint...\\n')\n",
    "    checkpoint = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "def load_checkpoint(model, optimizer, checkpoint_file):\n",
    "    print('loading checkpoint...')\n",
    "    model_dict = model.state_dict()\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = LEARNING_RATE # reset the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    load_checkpoint(generator, optimizer_generator, GENERATOR_CHECKPOINT)\n",
    "    load_checkpoint(discriminator, optimizer_discriminator, DISCRIMINATOR_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving checkpoint...\n",
      "\n",
      "\n",
      "saving checkpoint...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(train_dataloader, total=len(train_dataloader), leave=False)\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for batch_index, (masked_image, mask_true_value) in enumerate(loop):\n",
    "        masked_image = masked_image.to(DEVICE)\n",
    "        mask_true_value = mask_true_value.to(DEVICE)\n",
    "        \n",
    "        train_generator_batch(masked_image, mask_true_value)\n",
    "        train_discriminator_batch(masked_image, mask_true_value)\n",
    "        \n",
    "        loop.set_description(f'Epoch [{epoch}/{EPOCHS}]')\n",
    "        \n",
    "    generator.eval()\n",
    "    if SAVE_MODEL:\n",
    "        save_checkpoint(generator, optimizer_generator, GENERATOR_CHECKPOINT)\n",
    "        save_checkpoint(discriminator, optimizer_discriminator, DISCRIMINATOR_CHECKPOINT)\n",
    "    \n",
    "    # ----- Visualize the training progress ----- #\n",
    "    if VISUALIZE_TRAINING: \n",
    "        with torch.no_grad():\n",
    "            upper_left = (IMAGE_SIZE-MASK_SIZE)//2\n",
    "            \n",
    "            generated_mask_true_values = generator(training_visualization_images[0].to(DEVICE))\n",
    "            \n",
    "            filled_with_reals = fill_mask(training_visualization_images[0], training_visualization_images[1], upper_left, upper_left)\n",
    "            filled_with_fakes = fill_mask(training_visualization_images[0], generated_mask_true_values, upper_left, upper_left)\n",
    "            \n",
    "            grid = make_grid(torch.cat((training_visualization_images[0], filled_with_reals, filled_with_fakes)), nrow=5)\n",
    "            grid = grid.permute(1,2,0).cpu().numpy()\n",
    "            plt.imshow(grid)\n",
    "            _ = plt.xticks([])\n",
    "            _ = plt.yticks([75, 200, 325], ['masked', 'filled', 'generated'])\n",
    "            plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
